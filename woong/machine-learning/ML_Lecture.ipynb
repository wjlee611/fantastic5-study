{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "> Lectured by [Sung Kim](https://www.youtube.com/playlist?list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)\n",
    "\n",
    "> Study and wrote by [Woong](https://github.com/wjlee611)\n",
    "\n",
    "`❗️ 아래 강의 정리는 tf v.1 기준으로 작성하고, 다른 파일(예제, 코드)은 tf v.2 기준으로 작성되었습니다 ❗️`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "### Basic understanding of machine learning algorithm\n",
    "- Linear regression, Logistic regression (classification)\n",
    "- Neural networks, Convolutional neural network (CNN), Recurrent neural network (RNN)\n",
    "\n",
    "### Solve your problems using machine learning tools\n",
    "- Tensorflow and Python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 정리\n",
    "\n",
    "| 용어 | 설명 | 학습 목표? |\n",
    "|---|---|---|\n",
    "| `Supervised Learning` | 지도 학습: 문제와 답, 태그가 붙어있는 training set으로 학습시키는 방법 | ✔ |\n",
    "| `Unsupervised Learning` | 비지도 학습: 문제는 있지만, 답, 태그는 없는 데이터로 스스로 학습하는 방법 | |\n",
    "| `Tensor rank` | 차원 개념. 0(Scalar), 1(Vector), 2(Metrix), 3(3-Tensor), n(n-Tensor) ... | ✔ |\n",
    "| `Tensor shape` | Rank 0: [], Rank 1: [D0], Rank 2: [D0, D1], Rank 3: [D0, D1, D2] ... | ✔ |\n",
    "| `Cost(Loss) function` | 가설-실제 데이터 사이의 차이를 계산하는 함수 (Loss가 작을수록 정확하다는 뜻) | ✔ |\n",
    "\n",
    "\n",
    "\n",
    "### 예시 (보충설명)\n",
    "\n",
    "- `Tensor shape`\n",
    "\n",
    "    a = [[1, 2], [4, 5], [7, 8]], shape of a is [2, 3]\n",
    "\n",
    "- `Cost function`\n",
    "\n",
    "    <img src=\"./images/cost_function_1.png\" width=\"400\" height=\"300\">\n",
    "    <img src=\"./images/cost_function_2.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wjlee/opt/anaconda3/envs/py37_ML/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# tf v.1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "print(tf.__version__)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 17:12:33.793968: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-03 17:12:33.795151: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 10. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# This operation is added an a node to the default graph\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "# Start a TF session\n",
    "sess = tf.Session()\n",
    "# Run the operation and get result\n",
    "print(sess.run(hello))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=float32) + Tensor(\"Const_2:0\", shape=(), dtype=float32) = Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "3.0 + 4.0 = 7.0\n"
     ]
    }
   ],
   "source": [
    "# Computational Graph\n",
    "node1 = tf.constant(3.0, tf.float32) # 이렇게 데이터 타입을 정의할 수도 있음\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(node1, '+', node2, '=', node3)\n",
    "print(sess.run(node1), '+', sess.run(node2), '=', sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# Placeholder\n",
    "a = tf.placeholder(tf.float32);\n",
    "b = tf.placeholder(tf.float32);\n",
    "adder_node = a + b\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1, 3], b: [2, 4]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.819194 [0.8038217] [0.20722473]\n",
      "200 0.028672298 [1.1095617] [0.7044475]\n",
      "400 0.0073983157 [1.0556537] [0.89907247]\n",
      "600 0.0019089936 [1.0282702] [0.99793535]\n",
      "800 0.0004925822 [1.0143603] [1.0481544]\n",
      "1000 0.00012710287 [1.0072947] [1.073664]\n",
      "1200 3.279755e-05 [1.0037055] [1.0866219]\n",
      "1400 8.4636795e-06 [1.0018823] [1.0932039]\n",
      "1600 2.1837586e-06 [1.0009562] [1.096548]\n",
      "1800 5.636189e-07 [1.0004859] [1.098246]\n",
      "2000 1.4566831e-07 [1.0002471] [1.0991082]\n",
      "[7.100591]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random_normal([1]), name='weight') # [1]은 1차원 shape을 의미\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "x = tf.placeholder(tf.float32, shape=[None])\n",
    "y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# hypothesis\n",
    "hypothesis = x * w + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "# minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# initialize global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# fit the line\n",
    "for step in range(2001):\n",
    "    cost_val, w_val, b_val, _ = sess.run([cost, w, b, train], feed_dict={x: [1, 2, 3, 4, 5], y: [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 200 == 0:\n",
    "        print(step, cost_val, w_val, b_val)\n",
    "\n",
    "# testing model\n",
    "print(sess.run(hypothesis, feed_dict={x: [6]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "# Our hypothesis for linear modle X * W\n",
    "hypothesis = X * W\n",
    "# cost / loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initialize global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Variables for plotting cost function\n",
    "W_val = []\n",
    "cost_val = []\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.6175117 [-0.27762437]\n",
      "1 2.1667588 [0.31860036]\n",
      "2 0.6163226 [0.63658684]\n",
      "3 0.1753095 [0.80617964]\n",
      "4 0.0498658 [0.89662915]\n",
      "5 0.014184058 [0.94486886]\n",
      "6 0.0040345807 [0.97059673]\n",
      "7 0.001147613 [0.98431826]\n",
      "8 0.00032643077 [0.9916364]\n",
      "9 9.2851325e-05 [0.9955394]\n",
      "10 2.6411946e-05 [0.997621]\n",
      "11 7.5125463e-06 [0.9987312]\n",
      "12 2.1370006e-06 [0.9993233]\n",
      "13 6.0780536e-07 [0.9996391]\n",
      "14 1.7288636e-07 [0.99980754]\n",
      "15 4.9186898e-08 [0.99989736]\n",
      "16 1.3958759e-08 [0.9999453]\n",
      "17 3.9876653e-09 [0.9999708]\n",
      "18 1.1312563e-09 [0.99998444]\n",
      "19 3.1934277e-10 [0.9999917]\n",
      "20 9.1844754e-11 [0.9999956]\n"
     ]
    }
   ],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6266665\n",
      "10 2.361395\n",
      "20 1.5110468\n",
      "30 1.1918392\n",
      "40 1.0720135\n",
      "50 1.0270327\n",
      "60 1.0101477\n",
      "70 1.0038093\n",
      "80 1.00143\n",
      "90 1.0005368\n",
      "100 1.0002015\n"
     ]
    }
   ],
   "source": [
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    sess.run(train)\n",
    "    if step%10==0:\n",
    "        print(step, sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression (Multi variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  15048.322 \n",
      "Prediction:\n",
      " [39.74805 55.92739 51.05155 54.36862 45.59346]\n",
      "200 Cost:  4.9991026 \n",
      "Prediction:\n",
      " [148.74593 186.61111 179.98448 194.75804 145.20439]\n",
      "400 Cost:  4.5178742 \n",
      "Prediction:\n",
      " [148.90807 186.4996  180.03369 194.7973  145.05498]\n",
      "600 Cost:  4.085925 \n",
      "Prediction:\n",
      " [149.06155 186.39398 180.08023 194.83461 144.91331]\n",
      "800 Cost:  3.6982224 \n",
      "Prediction:\n",
      " [149.2069  186.294   180.1243  194.87012 144.77905]\n",
      "1000 Cost:  3.3501983 \n",
      "Prediction:\n",
      " [149.34448 186.19933 180.16599 194.9039  144.65176]\n",
      "1200 Cost:  3.0378096 \n",
      "Prediction:\n",
      " [149.47473 186.10968 180.20543 194.93604 144.5311 ]\n",
      "1400 Cost:  2.7574146 \n",
      "Prediction:\n",
      " [149.59802 186.0248  180.24275 194.9666  144.41672]\n",
      "1600 Cost:  2.5056825 \n",
      "Prediction:\n",
      " [149.71472 185.94443 180.27803 194.99567 144.30824]\n",
      "1800 Cost:  2.2796786 \n",
      "Prediction:\n",
      " [149.82518 185.86832 180.3114  195.02336 144.2054 ]\n",
      "2000 Cost:  2.0767841 \n",
      "Prediction:\n",
      " [149.92976 185.79628 180.343   195.04971 144.10791]\n"
     ]
    }
   ],
   "source": [
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize. Need a very small learning rate for this data set\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  112149.81 \n",
      "Prediction:\n",
      " [[-139.94304]\n",
      " [-176.13423]\n",
      " [-169.18985]\n",
      " [-188.05843]\n",
      " [-133.07443]]\n",
      "200 Cost:  10.588264 \n",
      "Prediction:\n",
      " [[156.9435 ]\n",
      " [181.16125]\n",
      " [182.61923]\n",
      " [195.09247]\n",
      " [139.53358]]\n",
      "400 Cost:  9.65724 \n",
      "Prediction:\n",
      " [[156.71178]\n",
      " [181.31892]\n",
      " [182.54674]\n",
      " [195.05165]\n",
      " [139.73068]]\n",
      "600 Cost:  8.820459 \n",
      "Prediction:\n",
      " [[156.49203]\n",
      " [181.46841]\n",
      " [182.4779 ]\n",
      " [195.01349]\n",
      " [139.91707]]\n",
      "800 Cost:  8.068358 \n",
      "Prediction:\n",
      " [[156.28363]\n",
      " [181.6101 ]\n",
      " [182.41255]\n",
      " [194.97775]\n",
      " [140.09329]]\n",
      "1000 Cost:  7.392114 \n",
      "Prediction:\n",
      " [[156.08594]\n",
      " [181.74446]\n",
      " [182.35051]\n",
      " [194.94434]\n",
      " [140.25993]]\n",
      "1200 Cost:  6.7840395 \n",
      "Prediction:\n",
      " [[155.89845]\n",
      " [181.87186]\n",
      " [182.29156]\n",
      " [194.91315]\n",
      " [140.41745]]\n",
      "1400 Cost:  6.237186 \n",
      "Prediction:\n",
      " [[155.7206 ]\n",
      " [181.9926 ]\n",
      " [182.2356 ]\n",
      " [194.88403]\n",
      " [140.56635]]\n",
      "1600 Cost:  5.745182 \n",
      "Prediction:\n",
      " [[155.5519 ]\n",
      " [182.10713]\n",
      " [182.18243]\n",
      " [194.85692]\n",
      " [140.70709]]\n",
      "1800 Cost:  5.3024397 \n",
      "Prediction:\n",
      " [[155.39183]\n",
      " [182.21573]\n",
      " [182.13193]\n",
      " [194.83165]\n",
      " [140.8401 ]]\n",
      "2000 Cost:  4.903896 \n",
      "Prediction:\n",
      " [[155.23994]\n",
      " [182.31871]\n",
      " [182.08394]\n",
      " [194.80815]\n",
      " [140.96577]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.6702502\n",
      "1000 0.44007078\n",
      "2000 0.37150264\n",
      "3000 0.31854612\n",
      "4000 0.2771282\n",
      "5000 0.24438341\n",
      "6000 0.21812832\n",
      "7000 0.19675563\n",
      "8000 0.17909874\n",
      "9000 0.16430919\n",
      "10000 0.15176491\n",
      "\n",
      "Hypothesis:  [[0.03170055]\n",
      " [0.16012755]\n",
      " [0.30943146]\n",
      " [0.77930576]\n",
      " [0.938259  ]\n",
      " [0.9797284 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.655311\n",
      "200 0.51613605\n",
      "400 0.41411433\n",
      "600 0.33548403\n",
      "800 0.261424\n",
      "1000 0.22929847\n",
      "1200 0.20794082\n",
      "1400 0.19019827\n",
      "1600 0.17519727\n",
      "1800 0.16234054\n",
      "2000 0.15119883\n",
      "--------------\n",
      "[[5.2269711e-03 9.9476296e-01 1.0060972e-05]] [1]\n",
      "--------------\n",
      "[[0.8495701  0.13347329 0.01695666]] [0]\n",
      "--------------\n",
      "[[9.1505070e-09 3.3496637e-04 9.9966502e-01]] [2]\n",
      "--------------\n",
      "[[5.2269711e-03 9.9476296e-01 1.0060972e-05]\n",
      " [8.4957010e-01 1.3347329e-01 1.6956661e-02]\n",
      " [9.1505070e-09 3.3496637e-04 9.9966502e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)\n",
    "\n",
    "    print('--------------')\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08126dd0a5844298dee99d60528949c8812db41a2dddab4a2d4ba34fd01b96ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
